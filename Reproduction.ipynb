{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction: Planning in Dynamic Environments with Conditional Autoregressive Models\n",
    "\n",
    "**Abhinav Madahar (<abhinav.madahar@rutgers.edu>) &middot; Sungjin Ahn**\n",
    "\n",
    "Todo:\n",
    "\n",
    "- [x] Movement about the environment is continuous, but collision and goal checking is quantized to the nearest pixel.\n",
    "- [x] The agent (controlled by the model) is a $1 \\times 1$ object and the goal is $2 \\times 2$.\n",
    "- [x] Both are randomly placed.\n",
    "- [x] The goal is initialized with a velocity with random direction and fixed speed.\n",
    "- [x] The agent must reach the goal in a set time without hitting an obstacle.\n",
    "- [x] The agent can have 1 of 2 fixed speeds, 0.5 pixels/timestep or 1 pixel/timestep.\n",
    "- [x] At each timestep the agent has the choice of 8 actions. These actions indicate one of 8 equally spaced angles and a constant speed.\n",
    "- [x] The goal also reflects off world boundaries, like in billards.\n",
    "- [x] The environment has configurable height and width.\n",
    "- [x] The environment is divided into obstacle lanes which span the environment horizontally.\n",
    "- [x] At the beginning of each episode, the lanes are randomly assigned to carry 1 of 5 classes of obstacles and a direction of movement (left to right or right to left). \n",
    "- [x] Each obstacle class is parameterized by a color and a distribution which describes average obstacle speed and length. \n",
    "- [x] Obstacles maintain a constant speed after entering the environment, pass through the edges of the environment, and are deleted after their entire body exits the observable space.\n",
    "- [x] The number of obstacles introduced into the environment at each timestep is controlled by a Poisson distribution, configured by the level parameter.\n",
    "- [x] Encode the environment in a VAE-readable state.\n",
    "- [x] Make an encoder which reads this environment and compresses it to a continuous 36-vector to mimic the discrete 6x6x1 Z-representation.\n",
    "- [x] Make dataset of X values (flattened current frame) and y values (flattened next frame).\n",
    "- [x] Train the model.\n",
    "- [x] Make goal bounce off walls correctly\n",
    "- [x] https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- [ ] Make obstacles come in and out correctly\n",
    "- [ ] An agent receives a reward of +20 for entering the same pixel-space as the goal and a âˆ’20 reward for entering the same pixel-space as an obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from math import sqrt\n",
    "from collections import namedtuple\n",
    "from enum import Enum\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import medfilt\n",
    "from torch import autograd, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Environment\n",
    "\n",
    "We first make the environment.\n",
    "These are some, but not all, of the parts I need to finish to make the environment.\n",
    "\n",
    "At first, the agent will be a simple VAE.\n",
    "\n",
    "I want the `Environment` class to be completely deterministic, assuming that the model is deterministic.\n",
    "The randomness, like for placing the goal and agent, are separated into other functions.\n",
    "We also create an enum to keep track of whether the current game is running, has been won, or has been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested):\n",
    "    values = []\n",
    "    for element in nested:\n",
    "        try:\n",
    "            values = values.concat(flatten(element))\n",
    "        except:\n",
    "            values.append(element)\n",
    "    return values\n",
    "                    \n",
    "def sgn(val):\n",
    "    if val > 0:\n",
    "        return 1\n",
    "    elif val == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "class Vector2D:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __add__(self, v):\n",
    "        return Vector2D(self.x + v.x, self.y + v.y)\n",
    "    \n",
    "    def __mul__(self, a):\n",
    "        return Vector2D(a * self.x, a * self.y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(type(self).__name__, self.x, self.y)\n",
    "    \n",
    "    def __eq__(self, v):\n",
    "        return self.x == v.x and self.y == v.y\n",
    "\n",
    "class Position(Vector2D): \n",
    "    pass\n",
    "\n",
    "class Velocity(Vector2D): \n",
    "    pass\n",
    "        \n",
    "def random_position(height, width):\n",
    "    \"\"\"Generates a psuedorandom position where 0 <= x < height and 0 <= y < width. The x and y are integers.\"\"\"\n",
    "    return Position(random.randrange(height), random.randrange(width))\n",
    "\n",
    "def random_velocity(speed):\n",
    "    theta = random.uniform(-1, 1) * math.pi\n",
    "    return Velocity(speed * math.cos(theta), speed * math.sin(theta))\n",
    "\n",
    "class GameState(Enum):\n",
    "    ONGOING = 0\n",
    "    WON = 1\n",
    "    COLLIDED = 2\n",
    "    TIMEOUT = 3\n",
    "    \n",
    "class Direction(Enum):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "    \n",
    "ObstacleClass = namedtuple('ObstacleClass', ['length', 'speed'])\n",
    "Obstacle = namedtuple('Obstacle', ['obstacle_class', 'position'])\n",
    "\n",
    "def obstacle_from_class_id(id):\n",
    "    \"\"\"The `id` argument is the class ID in [0, 1, 2, 3, 4].\"\"\"\n",
    "    obstacle_classes = [\n",
    "        ObstacleClass(3, 1),\n",
    "        ObstacleClass(3, 2),\n",
    "        ObstacleClass(6, 1),\n",
    "        ObstacleClass(6, 2),\n",
    "        ObstacleClass(1.5, 0.5),\n",
    "    ]\n",
    "    return obstacle_classes[id]\n",
    "\n",
    "def random_obstacle_lanes(n_lanes):\n",
    "    return [obstacle_from_class_id(random.randrange(5)) for _ in range(n_lanes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make the `Environment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, height, width, agent_pos, goal_pos, agent_speed, goal_vel, max_time, level):\n",
    "        \"\"\"\n",
    "            Initialize the environment.\n",
    "            \n",
    "            Args:\n",
    "                height: int, the height of the environment.\n",
    "                width: int, the width of the environment.\n",
    "                agent_pos: Position, the initial position of the agent.\n",
    "                goal_pos: Position, the initial position of the goal; the goal is 2x2, so the goal_pos is its bottom-left corner.\n",
    "                agent_speed: float, the speed (not velocity) of the agent. The model controls the direction.\n",
    "                goal_vel: Velocity, the velocity of the goal at the start of the experiment. This can change if it hits a wall.\n",
    "                max_time: int, the maximum number of timesteps which can elapse before an automatic loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        assert 0 <= agent_pos.x <= width\n",
    "        assert 0 <= agent_pos.y <= height\n",
    "        assert 0 <= goal_pos.x <= width - 1\n",
    "        assert 0 <= goal_pos.y <= height - 1  # ditto\n",
    "        self.agent_pos = agent_pos\n",
    "        self.goal_pos = goal_pos\n",
    "\n",
    "        self.agent_speed = agent_speed\n",
    "        self.goal_vel = goal_vel\n",
    "        \n",
    "        self.max_time = max_time\n",
    "        self.time = 0\n",
    "        \n",
    "        # randomly assign obstacle lanes for each lane.\n",
    "        # note that a single lane can have multiple obstacles, so we have a list of obstacles in each lane\n",
    "        n_obstacle_lanes = self.height\n",
    "        self.n_obstacle_lanes = n_obstacle_lanes\n",
    "        self.obstacle_classes = random_obstacle_lanes(n_obstacle_lanes)\n",
    "        self.obstacle_directions = [Direction(random.randrange(2)) for _ in range(n_obstacle_lanes)]\n",
    "        self.obstacle_lanes = [[] for _ in range(n_obstacle_lanes)]\n",
    "        \n",
    "        self.level = level\n",
    "        self.add_obstacles()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Environment(height={}, width={}, agent_pos={}, goal_pos={}, agent_speed={}, goal_vel={})'.format(\n",
    "            self.height, self.width, self.agent_pos, self.goal_pos, self.agent_speed, self.goal_vel)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"An ASCII diagram of the environment.\"\"\"\n",
    "\n",
    "        board = '|' + '-' * self.width + '|\\n'\n",
    "        for row in range(self.height):\n",
    "            board += '|'\n",
    "            for col in range(self.width):\n",
    "                if Position(row, col) == self.rounded(self.agent_pos):\n",
    "                    board += 'a'\n",
    "                elif Position(row, col) in self.hitbox(self.goal_pos, (2, 2)):\n",
    "                    board += 'g'\n",
    "                else:\n",
    "                    board += ' '\n",
    "            board += '|\\n'\n",
    "        board += '|' + '-' * self.width + '|'\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def move(self):\n",
    "        \"\"\"Makes all the objects move in a single timestep and returns the game state.\"\"\"\n",
    "        \n",
    "        def move_object(obj, vel):\n",
    "            # instead of making a good solution, I made an easy one.\n",
    "            # we move the object by velocity * dt where dt is a small scalar.\n",
    "            # if we go out of bounds, then we flip the coordinate(s) which are out of bounds\n",
    "            dt = 0.01\n",
    "            for _ in range(int(1 / dt)):\n",
    "                obj += vel * dt\n",
    "                \n",
    "                if obj.x < 0:\n",
    "                    vel.x = abs(vel.x) * 1  # make sure to go right\n",
    "                elif obj.x > self.width:\n",
    "                    vel.x = abs(vel.x) * -1  # make sure to go left\n",
    "                    \n",
    "                if obj.y < 0:\n",
    "                    vel.y = abs(vel.y) * 1  # make sure to go up\n",
    "                elif obj.y > self.height:\n",
    "                    vel.y = abs(vel.y) * -1  # make sure to go down\n",
    "\n",
    "            \n",
    "            return obj\n",
    "        \n",
    "        # we move the agent and goal\n",
    "        self.agent_pos = move_object(self.agent_pos, self.model_decision())\n",
    "        self.goal_pos = move_object(self.goal_pos, self.goal_vel)\n",
    "\n",
    "        # we move the obstacles\n",
    "        for lane, direction in zip(self.obstacle_lanes, self.obstacle_directions):\n",
    "            for j, obstacle in enumerate(lane):\n",
    "                speed     = obstacle.obstacle_class.speed\n",
    "                direction = 1 if direction == Direction.RIGHT else -1\n",
    "                obstacle.position.y += speed * direction\n",
    "\n",
    "        self.remove_out_of_bounds_obstacles()\n",
    "        self.add_obstacles()\n",
    "        \n",
    "        self.time += 1\n",
    "        \n",
    "        if self.rounded(self.agent_pos) in self.hitbox(self.goal_pos, [2, 2]):\n",
    "            return GameState.WON\n",
    "        \n",
    "        if self.time >= self.max_time:\n",
    "            return GameState.TIMEOUT\n",
    "        \n",
    "        return GameState.ONGOING\n",
    "\n",
    "    def hitbox(self, position, shape: [int, int]):\n",
    "        \"\"\"\n",
    "        Returns an array of all the positions which are in this element's hitbox.\n",
    "        Note that shape = [length in x, height in y].\n",
    "        \"\"\"\n",
    "        \n",
    "        position = self.rounded(position)\n",
    "        hitboxes = []\n",
    "        for offset_y in range(ceil(shape[0])):\n",
    "            for offset_x in range(ceil(shape[1])):\n",
    "                hitboxes.append(Position(position.x + offset_x, position.y + offset_y))\n",
    "        return hitboxes\n",
    "\n",
    "    def rounded(self, position):\n",
    "        return Position(round(position.x), round(position.y))\n",
    "    \n",
    "    def model_decision(self):\n",
    "        \"\"\"Get the model's decision on where to move given the current environment using integers 0 through 7.\"\"\"\n",
    "        # until I replicate the model, it will always select action 0\n",
    "        decision = 0\n",
    "        decisions = [\n",
    "            Velocity(-1/sqrt(2), 1/sqrt(2)),  Velocity(0, 1),  Velocity(-1/sqrt(2), -1/sqrt(2)),\n",
    "            Velocity(-1, 0),                                   Velocity(0, 1),\n",
    "            Velocity(-1/sqrt(2), -1/sqrt(2)), Velocity(0, -1), Velocity(-1/sqrt(2), -1/sqrt(2)),\n",
    "        ]\n",
    "        return decisions[decision] * self.agent_speed\n",
    "    \n",
    "    def add_obstacles(self):\n",
    "        \"\"\"Add obstacles based on a Poisson distribution.\"\"\"\n",
    "        \n",
    "        # decide into which lanes to add obstacles\n",
    "        number_obstacles_to_introduce = np.random.poisson(lam=self.level)\n",
    "        obstacle_lanes = list(range(self.n_obstacle_lanes))\n",
    "        random.shuffle(obstacle_lanes)\n",
    "        lanes_in_which_we_add_obstacles = obstacle_lanes[:number_obstacles_to_introduce]\n",
    "        \n",
    "        for lane in lanes_in_which_we_add_obstacles:\n",
    "            end = 0 if self.obstacle_directions[lane] == Direction.RIGHT else self.width\n",
    "            obstacle = Obstacle(self.obstacle_classes[lane], Position(lane, end))\n",
    "            self.obstacle_lanes[lane].append(obstacle)\n",
    "        \n",
    "    def remove_out_of_bounds_obstacles(self):\n",
    "        for lane, direction in zip(self.obstacle_lanes, self.obstacle_directions):\n",
    "            for j, obstacle in enumerate(lane):\n",
    "                left_moving_and_out_of_bounds = direction == Direction.LEFT and obstacle.position.x + obstacle.obstacle_class.length < 0\n",
    "                right_moving_and_out_of_bounds = direction == Direction.RIGHT and obstacle.position.x + obstacle.obstacle_class.length > self.width\n",
    "                \n",
    "                if left_moving_and_out_of_bounds or right_moving_and_out_of_bounds:\n",
    "                    lane.pop(j)\n",
    "\n",
    "    def model_readable_representation(self):\n",
    "        \"\"\"Represents the environment using a torch tensor. A point is 0 if empty, 1 if an obstacle is there, and 2 for goal.\"\"\"\n",
    "        \n",
    "        # we have to increment because (self.width, self.height) is a valid point on the board\n",
    "        rep = np.zeros([self.width+1, self.height+1], dtype=np.float64)\n",
    "        \n",
    "        obstacles = []\n",
    "        for lane in self.obstacle_lanes:\n",
    "            for obstacle in lane:\n",
    "                obstacles.append(obstacle)\n",
    "        obstacle_points = []\n",
    "        for obstacle in obstacles:\n",
    "            for point in environment.hitbox(obstacle.position, [obstacle.obstacle_class.length, 1]):\n",
    "                obstacle_points.append(point)\n",
    "        \n",
    "        for position in obstacle_points:\n",
    "            # obstacles can sometimes go beyond the bounds along the x-axis, \n",
    "            # which is valid but makes it impossible to put them on the representation.\n",
    "            if 0 <= position.x <= self.width and 0 <= position.y <= self.height:\n",
    "                rep[position.x][position.y] = 0.5\n",
    "\n",
    "        goal_hitpoints = self.hitbox(self.goal_pos, [2, 1])\n",
    "        for position in goal_hitpoints:\n",
    "            if 0 <= position.x < self.width and 0 <= position.y <= self.height:\n",
    "                rep[position.x][position.y] = 1\n",
    "        \n",
    "        rep = torch.from_numpy(rep)\n",
    "        \n",
    "        return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make an environment and see what it looks like when it's run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 45\n",
    "width = 45\n",
    "max_time = 407\n",
    "goal_speed = 1\n",
    "agent_pos = random_position(height, width)\n",
    "goal_pos = random_position(height-1, width-1)\n",
    "agent_speed = 1\n",
    "goal_vel = random_velocity(goal_speed)\n",
    "level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "environment = Environment(height, width, agent_pos, goal_pos, agent_speed, goal_vel, max_time, level)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(0, 45), ylim=(0, 45))\n",
    "a = np.random.random((45, 45))\n",
    "cmap = LinearSegmentedColormap.from_list('mycmap', ['purple', 'cyan', 'yellow'])\n",
    "im = plt.imshow(a, interpolation='none', cmap=cmap)\n",
    "\n",
    "def animate(i):\n",
    "    frame = environment.model_readable_representation()\n",
    "    a = im.get_array()\n",
    "    a = frame\n",
    "    im.set_array(a)\n",
    "    environment.move()\n",
    "\n",
    "    return [im]\n",
    "    \n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, frames=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf5e963b38>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(environment.model_readable_representation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Model\n",
    "\n",
    "The model takes in 5 states of the environment and predicts the next state.\n",
    "It's a VQ-VAE.\n",
    "\n",
    "We first create the environment because the model must fit its environment, not the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the hyperparameters.\n",
    "We use a `Z_dim` of 36 because the final model will use a 6x6x1 Z-representation instead of a continuous real vector.\n",
    "We set `h_dim` to 200 because it seems like a good number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = (width + 1) * (height + 1)\n",
    "h_dim = 200\n",
    "Z_dim = 36\n",
    "minibatch_size = 1\n",
    "lr = 0.001\n",
    "n_train_epochs = 3\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make the model itself, which is a simple encoder-decoder model.\n",
    "\n",
    "We use Xavier initialization for the weights.\n",
    "The encoder is a 1-hidden-layer feed forward neural network, and the decoder is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    input_dim = size[0]\n",
    "    xavier_stdev = 1 / np.sqrt(input_dim / 2)\n",
    "    return Variable(torch.randn(*size) * xavier_stdev, requires_grad=True)\n",
    "\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "Whh = xavier_init(size=[h_dim, h_dim])\n",
    "bhh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wxh + bxh)\n",
    "    h = nn.relu(h @ Whh + bhh)\n",
    "    z_mu = h @ Whz_mu + bhz_mu\n",
    "    z_var = h @ Whz_var + bhz_var\n",
    "    max_z_var = 87.\n",
    "    # we have to keep the variance within a reasonable bound\n",
    "    # because we take the exponent of it when we find the loss,\n",
    "    # and it overflows past max_z_var\n",
    "    z_var = torch.clamp(z_var, min=-max_z_var, max=max_z_var)\n",
    "    return z_mu, z_var\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(minibatch_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = nn.relu(z @ Wzh + bzh)\n",
    "    X = torch.sigmoid(h @ Whx + bhx)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate a dataset of $X$ and $y$ values.\n",
    "Each $X$ value is a flattened frame, and the ground truth $y$ value is the next frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 100000\n",
    "X = np.zeros((n_data_points, (height + 1) * (width + 1)), dtype=np.float32)\n",
    "y = np.zeros((n_data_points, (height + 1) * (width + 1)), dtype=np.float32)\n",
    "\n",
    "for i in range(n_data_points // max_time + 1):\n",
    "    environment = Environment(height, width, agent_pos, goal_pos, agent_speed, goal_vel, max_time, level)\n",
    "    try:\n",
    "        for j in range(max_time):\n",
    "            rep = environment.model_readable_representation().flatten()\n",
    "            # we can go faster by using the current frame as the previous frame's next frame\n",
    "\n",
    "            try:\n",
    "                X[i*max_time+j] = rep\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                y[i*max_time+j-1] = rep\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            environment.move()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [Wxh, Whh, bhh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var, Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "losses = []\n",
    "losses_in_epoch = []\n",
    "losses_avg_per_epoch = []\n",
    "\n",
    "for epoch in range(n_train_epochs):\n",
    "    print('Beginning epoch {}'.format(epoch))\n",
    "    shuffled_indices = torch.randperm(len(X))\n",
    "    batches_indices = []\n",
    "    batch_indices = []\n",
    "    for index in shuffled_indices:\n",
    "        batch_indices.append(index)\n",
    "        if len(batch_indices) == batch_size:\n",
    "            batches_indices.append(batch_indices)\n",
    "            batch_indices = []\n",
    "            \n",
    "    losses_in_epoch = []\n",
    "    for batch_indices in batches_indices:\n",
    "        batch = Variable(X[batch_indices])\n",
    "        y_true = Variable(y[batch_indices])\n",
    "        \n",
    "        # Forward\n",
    "        z_mu, z_var = Q(batch)\n",
    "        z = sample_z(z_mu, z_var)\n",
    "        y_pred = P(z)\n",
    "\n",
    "        # Loss\n",
    "        recon_loss = nn.binary_cross_entropy(y_pred, y_true, size_average=False) / minibatch_size\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "        loss = recon_loss + kl_loss\n",
    "        \n",
    "        losses.append(loss)\n",
    "        losses_in_epoch.append(loss)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        solver.step()\n",
    "        \n",
    "    losses_avg_per_epoch.append(sum(losses_in_epoch) / len(batch_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(medfilt(losses))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the model's predicted next frame with the actual next frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(height, width, agent_pos, goal_pos, agent_speed, goal_vel, max_time, level)\n",
    "\n",
    "frame = environment.model_readable_representation()\n",
    "prediction = P(sample_z(*Q(environment.model_readable_representation().flatten().reshape(1, X_dim).float()))) \\\n",
    "             .reshape(int(X_dim ** 0.5), int(X_dim ** 0.5)).detach().numpy()\n",
    "environment.move()\n",
    "actual = environment.model_readable_representation()\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('mycmap', ['purple', 'yellow', 'blue'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle('Model Predictions', fontsize='xx-large')\n",
    "plt.subplot(231)\n",
    "plt.title('Current frame')\n",
    "plt.imshow(frame, cmap=cmap)\n",
    "plt.subplot(232)\n",
    "plt.title('Predicted frame')\n",
    "plt.imshow(prediction, cmap=cmap)\n",
    "plt.subplot(233)\n",
    "plt.title('Actual next frame')\n",
    "plt.imshow(actual, cmap=cmap)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(height, width, agent_pos, goal_pos, agent_speed, goal_vel, max_time, level)\n",
    "e1 = environment.model_readable_representation()\n",
    "environment.move()\n",
    "(e1 == environment.model_readable_representation()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
